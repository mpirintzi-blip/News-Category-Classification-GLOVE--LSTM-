











import kagglehub

# Download latest version
path = kagglehub.dataset_download("rmisra/news-category-dataset")

print("Path to dataset files:", path) 





#!pip install kagglehub
#!pip install tensorflow





import os
import csv
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import re      
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from sklearn.preprocessing import LabelEncoder
from wordcloud import WordCloud
from nltk import word_tokenize
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GlobalMaxPool1D, BatchNormalization, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, accuracy_score








#Εντοπίζουμε το αρχείο στον φάκελο που είναι αποθηκευμένο 
file_name = os.listdir(path)[0] 
full_path = os.path.join(path, file_name)

# Διάβασμα του dataset και απεικόνιση ενός ενδεικτικού τμήματος
df = pd.read_json(full_path, lines=True)
df.head()





df=df.dropna(how="any",axis=0) #διώχνω γραμμές που εχουν ΝΑ τιμές
df=df[['headline', 'short_description', 'category']] #dataframe μόνο με τις στήλες που χρειάζομαι
df.head()






category_counts=df['category'].value_counts() #καταμέτρηση των τιμών καθε κατηγορίας

#κατασκευή διαγράμματος
plt.figure(figsize=(12, 6))
category_counts.plot(kind='bar')
plt.title('Distribution of News Categories')
plt.xlabel('Category')
plt.ylabel('Number of Articles')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()









categories_top=['POLITICS','WELLNESS','ENTERTAINMENT','TRAVEL','STYLE & BEAUTY','PARENTING','HEALTHY LIVING','QUEER VOICES','FOOD & DRINK','BUSINESS']
df_new=df[df['category'].isin(categories_top)].copy() #στην category ελέγχει αν κάθε γραμμή είναι top_categorie και κρατάει μονο αυτές
df_new.head()


category_counts=df_new['category'].value_counts() #καταμέτρηση των τιμών κάθε κατηγορίας

#κατασκευή διαγράμματος
plt.figure(figsize=(12, 6))
category_counts.plot(kind='bar')
plt.title('Distribution of TOP 10 Categories')
plt.xlabel('Category')
plt.ylabel('Number of Articles')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()









def clean_text(text):
    text=str(text).lower()   #όλα τα γράμματα μικρά
    text=re.sub('[%s]'%re.escape(string.punctuation),'',text)  #διαγραφή σημείων στοίξης
    text=re.sub('\n','',text)  #αφαιρεί αλλαγές γραμμής
    text=re.sub('\w*\d\w*','',text) #αφαιρεί συνδιασμούς αριθμών-λέξεων π.χ.12iphone
    text=re.sub(r'http\S+|www\S+|https\S+', '', text)#αφαιρε΄΄ί  urls
    text=re.sub(r'\S+@\S+', '', text)  #αφαιρεί emails
    return text





#nltk.download('stopwords')
stop_words=stopwords.words('english')

def remove_stopwords(text):
     #έλεγχος κάθε λέξης αν ανήκει στην λίστα με τα stop_words αν όχι την κρατάμε.
    text=' '.join(word for word in text.split(' ') if word not in stop_words) 
    return text

    






stemmer=nltk.SnowballStemmer('english')
def stemm_text(text):
    text=' '.join(stemmer.stem(word) for word in text.split(' '))
    return text





def preprocess_data(text):
    text=clean_text(text)
    text=remove_stopwords(text)
    text=stemm_text(text)

    return text

df_new['combined_text']=df_new['headline']+" "+df_new['short_description']
df_new['text_clean']=df_new['combined_text'].apply(preprocess_data)
df_new.head()

    
    





le=LabelEncoder()
le.fit(df_new['category'])
df_new['category_encoded']=le.transform(df_new['category']) #μετατροπή των κατηγοριών σε αριθμούς
df_new.head()








#!pip install wordcloud


categories=df_new['category'].unique() #παίρνει τις κατηγορίες μια φορά την κάθε μια

plt.figure(figsize=(20,10))

for i,cat in enumerate(categories,1):  #loop για να βάζουμε την κάθε κατηγοριά σε καθε επανάληψη στην σωστή θέση(αρχίζει απο 1) στο γράφημα.
    text=' '.join(df_new[df_new['category']==cat]['text_clean'])
    
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    
    plt.subplot(2,5,i)#2 σειρες,5 στηλες,το i=θέση στο γράφημα
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(cat, fontsize=16)
plt.tight_layout()
plt.show()

    





texts=df_new['text_clean']
category=df_new['category_encoded']








tokenizer=Tokenizer()
tokenizer.fit_on_texts(texts)    # σάρωση της texts και καταγραφή την συχνότητα των λέξεων 

vocab_length=len(tokenizer.word_index)+1
vocab_length     #πόσες μοναδικές λέξεις εχει το λεξικό


#nltk.download('punkt_tab')





def embed(corpus):
    return tokenizer.texts_to_sequences(corpus) # μετατροπή κάθε λέξης με το ID της από το word_index.

#'σπάμε' την κάθε προταση σε tokens ,μετράμε το μέγεθος και αποθηκεύουμε αυτή με τα περισσότερα tokens.
longest_train=max(texts,key=lambda sentence:len(word_tokenize(sentence)))

##Ο αριθμός που θα προκύψει ορίζει το μέγεθος του πίνακα.
length_long_sentence=len(word_tokenize(longest_train))

train_padded_sequences=pad_sequences(


    embed(texts),
    length_long_sentence,
    padding='post' #padding με μηδενικά στο τέλος όπου χρειάζεται
)
train_padded_sequences










embeddings_dictionary=dict() 

embedding_dim=100 #θα επιλέξουμε τα 100d δηλαδή 100 χαρακτηριστικά για κάθε λέξη.
path_to_glove = 'glove.6B.100d.txt' # το αρχείο GLOVE

with open(path_to_glove, encoding='utf-8') as fp:
    for line in fp: #διάβασμα του αρχείου
        records = line.split() #χωρίζει την κάθε γραμμή σε στοιχεία
        word = records[0] #η λε΄ξη πριν τα 100 χαρακτηριστικά της 
        vector_dimensions = np.asarray(records[1:], dtype='float32') #δημιουργία πίνακα με τα διανύσματα των λέξεων σε μορφή δεκαδικών
        embeddings_dictionary[word] = vector_dimensions #έχουμε αποθηκεύσει κάθε λέξη και το διάνυσμα της.







embedding_matrix=np.zeros((vocab_length,embedding_dim)) 
for word,index in tokenizer.word_index.items():  #για καθε  ζευγάρι λέξη-ID του λεξικού word_index
    embedding_vector=embeddings_dictionary.get(word) #παίρνω το διάνυσμα 100d της κάθε λέξης
    if embedding_vector is not None: #αν βρεθεί η λέξη στο GLOVE
        embedding_matrix[index]=embedding_vector #αποθήκευση του διανύσματος στην αντιστοιχή σειρα σύμφωνα με το ID.









X_train,X_test,y_train,y_test=train_test_split(
    train_padded_sequences, #δεδομένα εισόδου 
    df_new['category_encoded'].values, #δεδομένα εξόδου
    test_size=0.25,
    random_state=101 #επιβεβαίωση τυχαιοτητάς σε κάθε επανάληψη
   
    
)







def glove_lstm():
 
    model=Sequential()
    #FIRST LAYER
    model.add(Embedding(
        input_dim=embedding_matrix.shape[0],#αριθμός των γραμμών του embedding_matrix (61.881)
        output_dim=embedding_matrix.shape[1],#αριθμός των στηλών (100d) 
        weights=[embedding_matrix],# τροφοδοτεί έτοιμο τον πινακα embedding_matrix
        ))
   #SECOND LAYER        
    model.add(Bidirectional(LSTM(
        length_long_sentence,#πλήθος νευρώνων ανάλογα με την μεγαλύτερη πρόταση 
        return_sequences=True,
        recurrent_dropout=0.2 #το 20% των συνδέσεων δεν λαμβάνονται υπόψη
        )))
    model.add(GlobalMaxPool1D())
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(length_long_sentence,activation="relu"))
    model.add(Dropout(0.5))
    model.add(Dense(length_long_sentence,activation="relu"))        

    model.add(Dense(10,activation='softmax')) #
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model
model=glove_lstm()
model.build(input_shape=(None, length_long_sentence))
model.summary()
        

    






model=glove_lstm()

#εφαρμο΄γη βάρων στις κατηγορίες για κα΄λύτερη εκμάθηση
weights = class_weight.compute_class_weight('balanced', 
                                            classes=np.unique(y_train), 
                                            y=y_train)
dict_weights = dict(enumerate(weights))

#παρακολούθηση κάθε κύκλου  εκπαίδευσης
checkpoint=ModelCheckpoint(
    'model.h5',
    monitor='val_loss',
    verbose=1,
    save_best_only=True
)
#ρύθμιση ταχύτητας εκμάθησης σε περίπτωση που δεν υπάρχει βελτίωση  
reduce_lr = ReduceLROnPlateau(
    monitor = 'val_loss', 
    factor = 0.2, 
    verbose = 1, 
    patience = 5,                        
    min_lr = 0.001
)

#εκπαίδευση του μοντέλου
history=model.fit(
    X_train,
    y_train,
    epochs=7, #ποσες επαναλήψεις εκμάθησης του μοντέλου θα γίνουν
    batch_size=32,
    validation_data=(X_test,y_test), #δεδομένα επαλήθευσης
    class_weight=dict_weights,
verbose=1,
callbacks=[reduce_lr,checkpoint]
)











def plot_learning_curves(history,arr):
    fig,ax=plt.subplots(1,2,figsize=(20,5))
    for idx in range(2): # loop για κάθε γράφημα
        ax[idx].plot(history.history[arr[idx][0]])
        ax[idx].plot(history.history[arr[idx][1]])
        ax[idx].legend([arr[idx][0],arr[idx][1]],fontsize=18)
        ax[idx].set_xlabel('A ',fontsize=16)
        ax[idx].set_ylabel('B',fontsize=16)
        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)


plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])


names = ['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY', 
         'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK', 'BUSINESS']





import matplotlib.pyplot as plt
import seaborn as sns

def conf_matrix(matrix,category_names):
    plt.figure(figsize=(10, 8))
    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',xticklabels=category_names,yticklabels=category_names) 
    plt.xlabel('Predicted Label', fontsize=14)
    plt.ylabel('True Label', fontsize=14)
    plt.title('Confusion Matrix', fontsize=16)
    plt.show()


y_preds = np.argmax(model.predict(X_test),axis=1).astype("int32")
cm=metrics.confusion_matrix(y_test, y_preds) #δημιουργία πίνακα με τις κατηγορίες  και τις προβέψεις.
conf_matrix(cm,names)





def show_metrics(y_pred,y_true):
    print(f"Accuracy Score:{accuracy_score(y_true,y_pred):.3f}")
    print("\nClassification Report:")
    print(classification_report(y_true,y_pred,target_names=names))


preds=np.argmax(model.predict(X_test),axis=-1)
show_metrics(preds,y_test)




